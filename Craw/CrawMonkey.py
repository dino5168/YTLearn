import requests
from bs4 import BeautifulSoup
from pathlib import Path
from urllib.parse import urljoin


def fetch_page(url: str) -> str:
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.text


def parse_content(html: str, base_url: str):
    soup = BeautifulSoup(html, "html.parser")

    # é–å®šå…§å®¹å€åŸŸ <div class="rte">
    content_div = soup.find("div", class_="rte")

    # æŠ“æ‰€æœ‰åœ–ç‰‡ç¶²å€ï¼ˆçµ•å°åŒ–ï¼‰
    img_urls = []
    if content_div:
        for img in content_div.find_all("img"):
            src = img.get("src", "")
            if src.startswith("//"):
                src = "https:" + src
            elif src.startswith("/"):
                src = urljoin(base_url, src)
            img_urls.append(src)

    # æŠ“æ‰€æœ‰æ–‡å­—å…§å®¹
    text_parts = []
    if content_div:
        for p in content_div.find_all("p"):
            text = p.get_text(strip=True)
            if text:
                text_parts.append(text)

    return img_urls, "\n\n".join(text_parts)


def download_images(img_urls, output_dir: Path):
    output_dir.mkdir(parents=True, exist_ok=True)
    saved_paths = []

    for i, img_url in enumerate(img_urls, 1):
        try:
            img_name = f"image_{i:02d}" + Path(img_url).suffix.split("?")[0]
            img_path = output_dir / img_name
            response = requests.get(img_url, stream=True)
            response.raise_for_status()

            with open(img_path, "wb") as f:
                for chunk in response.iter_content(8192):
                    f.write(chunk)

            saved_paths.append(img_path)
            print(f"âœ… ä¸‹è¼‰åœ–ç‰‡ï¼š{img_path.name}")
        except Exception as e:
            print(f"âŒ ç„¡æ³•ä¸‹è¼‰åœ–ç‰‡ {img_url}ï¼š{e}")

    return saved_paths


def save_text(text: str, output_dir: Path):
    output_dir.mkdir(parents=True, exist_ok=True)
    text_path = output_dir / "content.txt"
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"âœ… æ–‡å­—å·²å„²å­˜ï¼š{text_path}")
    return text_path


def main():
    print("=== ğŸ•·ï¸ ç¶²é çˆ¬èŸ²å·¥å…·ï¼ˆåœ–æ–‡ç‰ˆï¼‰===")
    url = input("è«‹è¼¸å…¥ç›®æ¨™ç¶²å€ï¼š").strip()
    output_dir = Path(input("è«‹è¼¸å…¥è¼¸å‡ºè³‡æ–™å¤¾ï¼ˆä¾‹å¦‚ï¼šc:\\temp\\zzï¼‰ï¼š").strip())

    try:
        print("\nğŸ” æ“·å–ç¶²é å…§å®¹ä¸­...")
        html = fetch_page(url)
        img_urls, text = parse_content(html, url)

        # åœ–ç‰‡è™•ç†
        if img_urls:
            print(f"\nğŸ“¸ å…±æ‰¾åˆ° {len(img_urls)} å¼µåœ–ç‰‡ï¼Œé–‹å§‹ä¸‹è¼‰...")
            download_images(img_urls, output_dir)
        else:
            print("âš ï¸ æ‰¾ä¸åˆ°åœ–ç‰‡")

        # æ–‡å­—è™•ç†
        if text:
            save_text(text, output_dir)
        else:
            print("âš ï¸ æ‰¾ä¸åˆ°æ®µè½æ–‡å­—")

    except Exception as e:
        print("âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š", e)


if __name__ == "__main__":
    main()
