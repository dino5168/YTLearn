import asyncio
import edge_tts
import os
import re
from pydub import AudioSegment
import tempfile


def parse_srt(file_path):
    """è§£æ SRT æª”æ¡ˆï¼Œå›å‚³ [(åºè™Ÿ, é–‹å§‹ç§’æ•¸, çµæŸç§’æ•¸, æ–‡å­—)] çš„åˆ—è¡¨"""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # æ ¹æ“š SRT æª”æ ¼å¼åˆ†æ®µ
    pattern = r"(\d+)\s+(\d{2}:\d{2}:\d{2},\d{3})\s-->\s(\d{2}:\d{2}:\d{2},\d{3})\s+(.*?)(?=\n\n|\Z)"
    matches = re.findall(pattern, content, re.DOTALL)

    result = []
    for index, start, end, text in matches:
        # æ¸…é™¤è¡Œå…§æ›è¡Œ
        clean_text = text.replace("\n", " ").strip()
        # è½‰æ›æ™‚é–“æ ¼å¼ç‚ºç§’æ•¸
        start_seconds = time_to_seconds(start)
        end_seconds = time_to_seconds(end)
        result.append((int(index), start_seconds, end_seconds, clean_text))

    return result


def time_to_seconds(time_str):
    """å°‡ SRT æ™‚é–“æ ¼å¼ (HH:MM:SS,mmm) è½‰æ›ç‚ºç§’æ•¸"""
    time_part, ms_part = time_str.split(",")
    hours, minutes, seconds = map(int, time_part.split(":"))
    milliseconds = int(ms_part)

    total_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000
    return total_seconds


def seconds_to_time(seconds):
    """å°‡ç§’æ•¸è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    ms = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{ms:03d}"


async def generate_speech_segment(text, voice, temp_file):
    """ç”Ÿæˆå–®å€‹èªéŸ³ç‰‡æ®µ"""
    try:
        communicate = edge_tts.Communicate(text, voice=voice)
        await communicate.save(temp_file)
        return True
    except Exception as e:
        print(f"âŒ èªéŸ³ç”Ÿæˆå¤±æ•—: {e}")
        return False


async def srt_to_synced_speech(
    srt_path,
    voice="zh-TW-HsiaoChenNeural",
    output_file="synced_audio.mp3",
    method="stretch",  # "stretch" æˆ– "gap"
):
    """
    å°‡ SRT è½‰æ›ç‚ºæ™‚é–“åŒæ­¥çš„èªéŸ³æª”æ¡ˆ
    method:
    - "stretch": æ‹‰ä¼¸/å£“ç¸®éŸ³æª”ä»¥ç¬¦åˆæ™‚é–“è»¸
    - "gap": ä¿æŒåŸå§‹èªéŸ³é€Ÿåº¦ï¼Œç”¨éœéŸ³å¡«è£œé–“éš™
    """
    if not os.path.exists(srt_path):
        print(f"âŒ æ‰¾ä¸åˆ° SRT æª”æ¡ˆï¼š{srt_path}")
        return

    segments = parse_srt(srt_path)
    if not segments:
        print("âš ï¸ æœªè§£æåˆ°ä»»ä½•å­—å¹•æ®µè½")
        return

    print(f"ğŸ¤ é–‹å§‹è½‰æ›ï¼Œå…± {len(segments)} æ®µå­—å¹•...")
    print(f"ğŸ“Š æ–¹æ³•ï¼š{'æ™‚é–“æ‹‰ä¼¸' if method == 'stretch' else 'éœéŸ³å¡«è£œ'}")

    # è¨ˆç®—ç¸½æ™‚é•·
    total_duration = max(end for _, _, end, _ in segments)
    print(f"â±ï¸ ç¸½æ™‚é•·ï¼š{seconds_to_time(total_duration)}")

    # å‰µå»ºç©ºç™½éŸ³æª”ä½œç‚ºåŸºåº•
    final_audio = AudioSegment.silent(duration=int(total_duration * 1000))

    with tempfile.TemporaryDirectory() as temp_dir:
        for index, start_time, end_time, text in segments:
            if not text.strip():
                continue

            print(
                f"â¡ï¸ [{index:04d}] {seconds_to_time(start_time)} --> {seconds_to_time(end_time)}: {text[:30]}..."
            )

            # ç”ŸæˆèªéŸ³æª”æ¡ˆ
            temp_file = os.path.join(temp_dir, f"temp_{index}.mp3")
            success = await generate_speech_segment(text, voice, temp_file)

            if not success:
                continue

            try:
                # è¼‰å…¥ç”Ÿæˆçš„éŸ³æª”
                speech_audio = AudioSegment.from_mp3(temp_file)
                target_duration = (end_time - start_time) * 1000  # è½‰ç‚ºæ¯«ç§’

                if method == "stretch":
                    # æ–¹æ³•1: æ‹‰ä¼¸/å£“ç¸®éŸ³æª”ä»¥ç¬¦åˆæ™‚é–“è»¸
                    if len(speech_audio) != target_duration:
                        # è¨ˆç®—æ’­æ”¾é€Ÿåº¦å€ç‡
                        speed_ratio = len(speech_audio) / target_duration
                        if speed_ratio > 0.5 and speed_ratio < 2.0:  # åˆç†çš„é€Ÿåº¦ç¯„åœ
                            # èª¿æ•´éŸ³æª”é•·åº¦
                            speech_audio = speech_audio._spawn(
                                speech_audio.raw_data,
                                overrides={
                                    "frame_rate": int(
                                        speech_audio.frame_rate * speed_ratio
                                    )
                                },
                            ).set_frame_rate(speech_audio.frame_rate)
                        else:
                            # å¦‚æœé€Ÿåº¦å·®ç•°å¤ªå¤§ï¼Œè£åˆ‡æˆ–å¡«è£œ
                            if len(speech_audio) > target_duration:
                                speech_audio = speech_audio[: int(target_duration)]
                            else:
                                speech_audio = speech_audio + AudioSegment.silent(
                                    duration=int(target_duration - len(speech_audio))
                                )

                elif method == "gap":
                    # æ–¹æ³•2: ä¿æŒåŸå§‹èªéŸ³é€Ÿåº¦ï¼Œç”¨éœéŸ³å¡«è£œ
                    if len(speech_audio) > target_duration:
                        # å¦‚æœèªéŸ³å¤ªé•·ï¼Œè£åˆ‡
                        speech_audio = speech_audio[: int(target_duration)]
                    elif len(speech_audio) < target_duration:
                        # å¦‚æœèªéŸ³å¤ªçŸ­ï¼Œå¾Œé¢åŠ éœéŸ³
                        silence_duration = target_duration - len(speech_audio)
                        speech_audio = speech_audio + AudioSegment.silent(
                            duration=int(silence_duration)
                        )

                # å°‡éŸ³æª”æ’å…¥åˆ°æ­£ç¢ºçš„æ™‚é–“ä½ç½®
                start_ms = int(start_time * 1000)
                end_ms = int(end_time * 1000)

                # ç¢ºä¿ä¸è¶…å‡ºç¯„åœ
                if start_ms < len(final_audio) and end_ms <= len(final_audio):
                    # è¦†è“‹åˆ°æœ€çµ‚éŸ³æª”ä¸Š
                    final_audio = (
                        final_audio[:start_ms]
                        + speech_audio[: end_ms - start_ms]
                        + final_audio[end_ms:]
                    )

            except Exception as e:
                print(f"âŒ ç¬¬ {index} æ®µè™•ç†å¤±æ•—: {e}")

    # è¼¸å‡ºæœ€çµ‚éŸ³æª”
    try:
        final_audio.export(output_file, format="mp3")
        print(f"\nâœ… è½‰æ›å®Œæˆï¼è¼¸å‡ºæª”æ¡ˆï¼š{output_file}")
        print(f"ğŸ“ æª”æ¡ˆå¤§å°ï¼š{os.path.getsize(output_file) / 1024 / 1024:.1f} MB")
    except Exception as e:
        print(f"âŒ è¼¸å‡ºå¤±æ•—: {e}")


async def srt_to_individual_segments(
    srt_path, voice="zh-TW-HsiaoChenNeural", output_dir="timed_segments"
):
    """ç”Ÿæˆå€‹åˆ¥çš„æ™‚é–“åŒæ­¥éŸ³æª”ç‰‡æ®µ"""
    if not os.path.exists(srt_path):
        print(f"âŒ æ‰¾ä¸åˆ° SRT æª”æ¡ˆï¼š{srt_path}")
        return

    segments = parse_srt(srt_path)
    if not segments:
        print("âš ï¸ æœªè§£æåˆ°ä»»ä½•å­—å¹•æ®µè½")
        return

    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸ¤ é–‹å§‹è½‰æ›å€‹åˆ¥ç‰‡æ®µï¼Œå…± {len(segments)} æ®µå­—å¹•...")

    for index, start_time, end_time, text in segments:
        if not text.strip():
            continue

        duration = end_time - start_time
        filename = os.path.join(
            output_dir,
            f"{index:04d}_{seconds_to_time(start_time).replace(':', '-').replace(',', '-')}_duration-{duration:.1f}s.mp3",
        )

        print(
            f"â¡ï¸ [{index:04d}] {seconds_to_time(start_time)} --> {seconds_to_time(end_time)} ({duration:.1f}s): {text[:30]}..."
        )

        try:
            communicate = edge_tts.Communicate(text, voice=voice)
            await communicate.save(filename)
        except Exception as e:
            print(f"âŒ ç¬¬ {index} æ®µè½‰æ›å¤±æ•—: {e}")

    print(f"\nâœ… æ‰€æœ‰ç‰‡æ®µè½‰æ›å®Œæˆï¼è¼¸å‡ºè³‡æ–™å¤¾ï¼š{output_dir}")


def main():
    srt_path = input("è«‹è¼¸å…¥ SRT å­—å¹•æª”è·¯å¾‘: ").strip()

    print("\nè«‹é¸æ“‡èªéŸ³ï¼š")
    voices = {
        "1": ("zh-TW-HsiaoChenNeural", "å°ç£å¥³ç”Ÿ - å°é™³"),
        "2": ("zh-TW-YunJheNeural", "å°ç£ç”·ç”Ÿ - é›²å“²"),
        "3": ("zh-CN-XiaoxiaoNeural", "å¤§é™¸å¥³ç”Ÿ - æ›‰æ›‰"),
        "4": ("zh-CN-YunxiNeural", "å¤§é™¸ç”·ç”Ÿ - é›²å¸Œ"),
        "5": ("en-US-JennyNeural", "è‹±èªå¥³ç”Ÿ - Jenny"),
        "6": ("en-US-GuyNeural", "è‹±èªç”·ç”Ÿ - Guy"),
    }

    for key, (_, desc) in voices.items():
        print(f"{key}. {desc}")
    voice_choice = input("è«‹é¸æ“‡èªéŸ³ (1-6): ").strip()
    voice = voices.get(voice_choice, voices["1"])[0]

    print("\nè«‹é¸æ“‡è¼¸å‡ºæ¨¡å¼ï¼š")
    print("1. åˆä½µç‚ºå–®ä¸€åŒæ­¥éŸ³æª” (æ™‚é–“æ‹‰ä¼¸)")
    print("2. åˆä½µç‚ºå–®ä¸€åŒæ­¥éŸ³æª” (éœéŸ³å¡«è£œ)")
    print("3. å€‹åˆ¥ç‰‡æ®µæª”æ¡ˆ (å«æ™‚é–“è³‡è¨Š)")
    mode = input("è«‹é¸æ“‡æ¨¡å¼ (1-3): ").strip()

    try:
        if mode == "1":
            output_file = (
                input("è¼¸å‡ºæª”å (é è¨­: synced_audio_stretch.mp3): ").strip()
                or "synced_audio_stretch.mp3"
            )
            asyncio.run(srt_to_synced_speech(srt_path, voice, output_file, "stretch"))
        elif mode == "2":
            output_file = (
                input("è¼¸å‡ºæª”å (é è¨­: synced_audio_gap.mp3): ").strip()
                or "synced_audio_gap.mp3"
            )
            asyncio.run(srt_to_synced_speech(srt_path, voice, output_file, "gap"))
        elif mode == "3":
            output_dir = (
                input("è¼¸å‡ºè³‡æ–™å¤¾ (é è¨­: timed_segments): ").strip() or "timed_segments"
            )
            asyncio.run(srt_to_individual_segments(srt_path, voice, output_dir))
        else:
            print("âŒ ç„¡æ•ˆçš„é¸æ“‡")
            return

    except KeyboardInterrupt:
        print("\nç¨‹å¼å·²ä¸­æ–·ã€‚")
    except Exception as e:
        print(f"åŸ·è¡Œå¤±æ•—: {e}")


if __name__ == "__main__":
    main()
